<!-- saved from url=(0029)https://dreambooth.github.io/ -->
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>BLIP-Diffusion</title>
    <link href="./style.css" rel="stylesheet" />
  </head>

  <body>
    <div class="content">
      <h1>
        <strong
          >BLIP-Diffusion: Pre-trained Subject Representation for Controllable
          Text-to-Image Generation and Editing</strong
        >
      </h1>
      <p id="authors">
        <span><a href=""></a></span>
        <a href="https://scholar.google.com/citations?user=h5XtaUUAAAAJ&hl=en"
          >Dongxu Li</a
        >
        <a href="https://sites.google.com/site/junnanlics//">Junnan Li</a>
        <a href="https://sites.google.com/view/stevenhoi/home"
          >Steven C.H. Hoi</a
        >
        <br />
        <br />
        <span style="font-size: 24px">Salesforce AI Research </span>
      </p>
      <font size="+2">
        <p style="text-align: center">
          <a href="" target="_blank">[Paper]</a>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a
            href="https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion"
            target="_blank"
            >[Code]</a
          >
          <!-- &nbsp;&nbsp;&nbsp;&nbsp; (<font color="#C70039">new!</font>) -->
          <!-- <a href="https://github.com/google/dreambooth" target="_blank" -->
          <!-- >[Dataset]</a -->
          <!-- > -->
          <!-- &nbsp;&nbsp;&nbsp;&nbsp; -->
          <!-- <a
            href="https://dreambooth.github.io/DreamBooth_files/bibtex.txt"
            target="_blank"
            >[BibTeX]</a -->
          <!-- > -->
        </p>
      </font>
      <h3 style="text-align: center">
        <em
          >Bringing built-in multi-modal text-and-subject control to diffusion
          models.</em
        >
      </h3>
      <br />
      <img
        src="./data/images-in-paper/teaser-website.png"
        class="teaser-gif"
        style="width: 100%"
      /><br />
    </div>
    <div class="content">
      <h2 style="text-align: center">Abstract</h2>
      <p>
        Subject-driven text-to-image generation models create novel renditions
        of an input subject based on text prompts. Existing models suffer from
        lengthy fine-tuning and difficulties preserving the subject fidelity. To
        overcome these limitations, we introduce BLIP-Diffusion, a new
        subject-driven image generation model that supports multimodal control
        which consumes inputs of subject images and text prompts. Unlike other
        subject-driven generation models, BLIP-Diffusion introduces a new
        multimodal encoder which is pre-trained to provide subject
        representation. We first pre-train the multimodal encoder following
        BLIP-2 to produce visual representation aligned with the text. Then we
        design a subject representation learning task, called prompted context
        generation, which enables a diffusion model to leverage such visual
        representation and generates new subject renditions. Compared with
        previous methods such as DreamBooth, our model enables zero-shot
        subject-driven generation, and efficient fine-tuning for customized
        subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion
        can be flexibly combined with existing techniques such as ControlNet and
        prompt-to-prompt to enable novel subject-driven generation and editing
        applications.
      </p>
    </div>
    <div class="content">
      <h2 style="text-align: center">Background and Motivation</h2>
      <p>
        Natural language prompts are usually effective to specify generic image
        content. Yet, they can be inadequate to accurately describe highly
        customized visual concepts, or the novel ones that are not acquaint to
        the underlying text-to-image image. One example is to generate a cool
        oil painting of your pet puppey, who has unique back markings. Instead
        of describing the markings in text, it is more clarified to just show
        the model a few images of the puppey as conditions. Another case is that
        imagine you are using an AI interior design assistant to decorate your
        new house. Instead of describing the interior design style you like in
        text, it is more effective to show the model a few photos of the style
        you like. Both cases motivate the use of multimodal inputs, as a
        complementary source of information to guide the generation process.

        <br />
        <br />
        However, existing text-to-image generation models are not designed to
        support such multimodal conditions. This leads to inefficiency to adapt
        them in related applications, such as the task of subject-driven
        text-to-image generation. In addition, having multimodal conditions
        enhances the controllability of the generation process - highly
        desirable for real-world generative AI applications. As such, we propose
        BLIP-Diffusion, a new subject-driven image generation model with
        built-in support for multimodal conditions, bringing high-level
        controllability to diffusion models.
      </p>
    </div>
    <div class="content">
      <h2 style="text-align: center">
        Demo I: Subject-driven Text-to-Image Generation
      </h2>
      <p>
        Given a few images of a subject, our model can generate novel renditions
        of the subject based on text prompts. The following examples show such
        subject-driven text-to-image generation results on the DreamBooth
        dataset. Each row shows the results of a different subject. Benefiting
        from the pre-traiend subject representation, our model shows superior
        fine-tuning efficiency and high subject fidelity. For example, our model
        requires on average less than 80 steps for generic subjects, less than
        40 steps for certain subjects, as opposed to DreamBooth (600-1200 steps)
        and Textual Inversion (~3000 steps), a 10-20x higher fine-tuning
        efficiency.
      </p>
      <h3>Mini Challenge: Can You Guess It Right?</h3>
      <p>
        To showcase the subject fidelity, we create the following challenge,
        where in each row, we mix one geniune subject image with generations
        from our model. Can you guess which one is the geniune subject image?
        <strong>Click the image to find out the answer!</strong>
        Not challenging enough? Click "Challenge" for more!
      </p>
      <div class="select" style="margin-bottom: 10px">
        <!-- <span style="font-size: 16px">Display Mode: </span> -->
        <button class="mode_choose easyButton" disabled>High-Resolution</button>
        <button class="mode_choose hardButton">Challenge</button>
      </div>

      <div class="box">
        <div class="imageContainer"></div>
        <div class="imageContainer"></div>
        <div class="imageContainer"></div>
        <div class="imageContainer"></div>
        <div class="imageContainer"></div>
      </div>
    </div>
    <div class="content">
      <h2 style="text-align: center">Demo II: Subject-driven Image Editing</h2>
      <p>
        Our model can edit source images with subject-specific visuals. We refer
        this task as <i>subject-driven image editing</i>. The following examples
        show results of such applications. <strong>On the left</strong>, we show
        the source image we'd like to edit. <strong>On the right</strong>, we
        show images of subjects we'd like to edit the source image with. For
        example, in the first image of squirrel, we aim to edit the source image
        by replacing the dog with the squirrel. In the meantime, the background
        should be preserved. Note how postures of the subjects naturally fit
        into the source image. This shows that the editing process is not just a
        simple copy-and-paste.

        <br />
        <br />
        Click on each subject image to see the <i>zero-shot</i> editing results
        with different subjects.
      </p>

      <div class="demo2">
        <div class="org-image">
          <img src="data/images-in-paper/demo-editing/original.png" />
        </div>
        <div class="edited-img">
          <img src="data/images-in-paper/demo-editing/squirrel.jpg" />
          <img src="data/images-in-paper/demo-editing/bird.jpg" />
          <img src="data/images-in-paper/demo-editing/cat.jpg" />
          <img src="data/images-in-paper/demo-editing/dog.jpg" />
          <img src="data/images-in-paper/demo-editing/horse.jpg" />
          <img src="data/images-in-paper/demo-editing/leopard.jpg" />
          <img src="data/images-in-paper/demo-editing/lion.jpg" />
          <img src="data/images-in-paper/demo-editing/white-tiger.jpg" />
          <button
            class="mode_choose"
            id="reset"
            style="margin: 10px; padding: 8px 20px"
          >
            Reset Original
          </button>
        </div>
      </div>
      <p>
        We can also use fine-tuned model to enable editing with
        highly-customized subjects. The appearance of the subjects are well
        preserved. Also note the lighting on the subjects are nicely blended
        into the source image, e.g. the vase example.
      </p>

      <div class="showcase">
        <div class="image-box">
          <p style="margin: 0">source image</p>
          <img src="data/images-in-paper/demo-editing/cake-src.png" />
          <p>
            <i>a burger on the sea with sunset.</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">subject imagee</p>
          <img src="data/images-in-paper/demo-editing/cake-subj.png" />
          <p>
            <i>cake</i>
          </p>
        </div>

        <div class="image-box" style="margin-right: 20px">
          <p style="margin: 0">editing output</p>
          <img src="data/images-in-paper/demo-editing/cake-out.png" />
          <p>
            <i>a cake on the sea with sunset.</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">source image</p>
          <img src="data/images-in-paper/demo-editing/vase-src.png" />
          <p>
            <i>a vase with weed in the mountain.</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">subject image</p>
          <img src="data/images-in-paper/demo-editing/vase-subj.png" />
          <p>
            <i>vase</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">editing output</p>

          <img src="data/images-in-paper/demo-editing/vase-out.png" />
          <p>
            <i>a vase with weed in the mountain.</i>
          </p>
        </div>
      </div>
      <div class="showcase">
        <div class="image-box">
          <p style="margin: 0">source image</p>
          <img src="data/images-in-paper/demo-editing/flouffy-dog-src.png" />
          <p>
            <i>a dog, cyberpunk character ...</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">subject image</p>
          <img src="data/images-in-paper/demo-editing/flouffy-dog-subj.png" />
          <p>
            <i>dog</i>
          </p>
        </div>
        <div class="image-box" style="margin-right: 20px">
          <p style="margin: 0">editing output</p>
          <img src="data/images-in-paper/demo-editing/flouffy-dog-out.png" />
          <p>
            <i>a dog, cyberpunk character ...</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">source image</p>
          <img src="data/images-in-paper/demo-editing/unsplash-bird-src.png" />
          <p><i>a dog on the grass, oil painting.</i></p>
        </div>
        <div class="image-box">
          <p style="margin: 0">subject image</p>
          <img src="data/images-in-paper/demo-editing/unsplash-bird-subj.jpg" />
          <p>
            <i>bird</i>
          </p>
        </div>
        <div class="image-box">
          <p style="margin: 0">editing output</p>
          <img src="data/images-in-paper/demo-editing/unsplash-bird-out.png" />
          <p><i>a bird on the grass, oil painting.</i></p>
        </div>
      </div>
    </div>
    <div class="content">
      <h2 style="text-align: center">
        Demo III: Zero-shot Subject-driven Image Manipulation
      </h2>
      <p>
        Our model is able to extract subject features to guide the generation.
        In addition to applications of subject-driven generations and editing,
        we show that such pre-trained subject representation enables intriguing
        and useful applications of zero-shot image manipulation, including
        subject-driven style transfer and subject interpolation.
      </p>
      <p>
        <strong>Left</strong>: subject interpolation. It is also possible to
        blend two subject representation to generate subjects with a hybrid
        appearance. This can be achieved by traversing the embedding trajectory
        between subjects.
        <strong> Drag the slider to see the interpolation results. </strong>
      </p>
      <p>
        <strong>Right</strong>: subject-driven style transfer. When provided
        with a subject, the model can encode the appearance style of it and
        transfer to other subjects. We refer such an application as
        subject-driven style transfer.
        <strong>
          Click on different reference and guiding subjects to see the style
          transfer results.
        </strong>
      </p>

      <div class="img-slider">
        <div class="cat-main">
          <i
            ><p
              style="
                margin: 6px;
                margin-top: 15px;
                text-align: center;
                font-size: 1.5rem;
              "
            >
              Subject Interpolation
            </p></i
          >
          <img
            class="cat-img"
            id="interp-img1"
            src="./data/interpolation/31_x100y24.jpg"
            style="margin-top: 10px"
          />
          <div class="slider-container">
            <input
              type="range"
              min="0"
              max="75"
              value="31"
              class="slider"
              id="interp-range1"
            />
            <div class="img-container">
              <img
                class="first-img"
                src="./data/images-in-paper/demo2/dog7.jpg"
              />
              <img
                class="first-img"
                src="./data/images-in-paper/demo2/dog3.jpg"
              />
              <img
                class="first-img"
                src="./data/images-in-paper/demo2/cat2.jpg"
              />
              <img
                class="first-img"
                src="./data/images-in-paper/demo2/cat.jpg"
              />
            </div>
          </div>
        </div>
        <div class="cat-main">
          <p
            style="
              margin: 15.5px;
              margin-left: 100px;
              text-align: center;
              font-size: 1.5rem;
            "
          >
            <i>Subject-driven Style Transfer</i>
          </p>
          <div class="generater">
            <div class="input-image">
              <img
                class="left-img clicked-image"
                src="./data/images-in-paper/demo2/ref-subjects/sculpture.jpg"
              />
              <img
                class="left-img"
                src="./data/images-in-paper/demo2/ref-subjects/pot.jpeg"
              />
              <img
                class="left-img"
                src="./data/images-in-paper/demo2/ref-subjects/blip-logo.jpg"
              />
              <img
                class="left-img"
                src="./data/images-in-paper/demo2/ref-subjects/hf-logo.jpg"
              />
              <p style="text-align: center; margin: 5px">Reference Subjects</p>
            </div>
            <div class="output-image">
              <img
                style="width: 100%"
                ;
                class="cat-img"
                style="margin-top: 5px"
                id="generate-img"
                src="./data/images-in-paper/demo2/stylized-subjects/sculpture-fire.png"
              />
              <p style="margin: 11px; text-align: center">Guiding Subjects</p>
              <div
                class="img-container guiding-subjuects"
                style="padding-top: 0px"
              >
                <img
                  class="last-img clicked-image"
                  src="./data/images-in-paper/demo2/condition/fire.jpg"
                />
                <img
                  class="last-img"
                  src="./data/images-in-paper/demo2/condition/flower.jpg"
                />
                <img
                  class="last-img"
                  src="./data/images-in-paper/demo2/condition/vase.jpg"
                />
                <img
                  class="last-img"
                  src="./data/images-in-paper/demo2/condition/yarn.jpg"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- <!-- <div class="content">
      <h2>Background</h2>
      <p>
        Given a particular subject such as clock (shown in the real images on
        the left), it is very challenging to generate it in different contexts
        with state-of-the-art text-to-image models, while maintaining high
        fidelity to its key visual features. Even with dozens of iterations over
        a text prompt that contains a detailed description of the appearance of
        the clock (<em
          >"retro style yellow alarm clock with a white clock face and a yellow
          number three on the right part of the clock face in the jungle"</em
        >), the Imagen model [Saharia et al., 2022] can't reconstruct its key
        visual features (third column). Furthermore, even models whose text
        embedding lies in a shared language-vision space and can create semantic
        variations of the image, such as DALL-E2 [Ramesh et al., 2022], can
        neither reconstruct the appearance of the given subject nor modify the
        context (second column). In contrast, our approach (right) can
        synthesize the clock with high fidelity and in new contexts (<em
          >"a [V] clock in the jungle"</em
        >).
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/background.png"
        style="width: 100%"
      />
      <br />
    </div> -->
    <div class="content">
      <h2 style="text-align: center">Approach</h2>
      <i><h3>Two-stage Pre-training</h3></i>
      <p>
        Central to our approach is the novel concept of
        <strong>pre-trained subject representation</strong>. Such representation
        aligns with text embeddings and in the meantime also encodes the subject
        appearance. This allows efficient fine-tuning of the model for
        high-fidelity subject-driven applications, such as text-to-image
        generation, editing and style transfer.
      </p>
      <p>
        To this end, we design a two-stage pre-training strategy to learn
        generic subject representation. In the first pre-training stage, we
        perform multimodal representation learning, which enforces BLIP-2 to
        produce text-aligned visual features based on the input image. In the
        second pre-training stage, we design a subject representation learning
        task, called prompted context generation, where the diffusion model
        learns to generate novel subject renditions based on the input visual
        features.
      </p>
      <p>
        To achieve this, we curate pairs of input-target images with the same
        subject appearing in different contexts. Specifically, we synthesize
        input images by composing the subject with a random background. During
        pre-training, we feed the synthetic input image and the subject class
        label through BLIP-2 to obtain the multimodal embeddings as subject
        representation. The subject representation is then combined with a text
        prompt to guide the generation of the target image.
      </p>
      <br />
      <img class="summary-img"/ src="./data/images-in-paper/arch.jpg"
      style="width: 100%" />
      <br />
      <i><h3>Controllable Generation and Editing</h3></i>
      <p>
        Our model introduces a multimodal conditioning mechanism for
        subject-control. In the meanwhile, the architecture is also compatible
        to integrate with established techniques built on top of the diffusion
        model, such as ControlNet and prompt-to-prompt for controllable
        generation and editing.
      </p>
      <img class="summary-img"/ src="./data/images-in-paper/inference.jpg"
      <br />
      <p>
        For ControlNet, we attach the U-Net of the pre-trained ControlNet to
        that of BLIP-Diffusion via residuals. In this way, the model takes into
        account the input structure condition, such as edge maps and depth maps,
        in addition to the subject cues. Since our model inherits the
        architecture of the original latent diffusion model, we observe
        satisfying generations using off-the-shelf integration with pre-trained
        ControlNet without further training
      </p>
      <p>
        For subject-driven image editing, we extract automatically an editing
        region to edit using attention maps. We mix the denoising latents at
        each step based on the extracted editing mask. Namely, latents of the
        unedited regions are from the original generation whereas latents of the
        edited regions are from the subject-driven generation. In this way, we
        obtain the edited image with subject-specific visuals while also
        preserving the unedited regions.
      </p>
    </div>
    <div class="content">
      <h2 style="text-align: center">Ethical and Responsible Use</h2>
      <p>
        Image generation models are susceptible to be used as tools for
        generating false content or prompting misinformation. Subject-driven
        generation could be misused as a tool for generating fake image of
        individuals. To mitigate this issue, our model has been trained on
        generic objects where person-related subjects have been purposely
        removed from the training data. This makes the model weaker at
        generating fake images using person as subject control.
      </p>
      <p>
        Our model is built using the pre-trained Stable Diffusion model trained
        on web-scraped datasets. Therefore, our model inherits some shortcomings
        from Stable Diffusion, such as generating biased contents with social
        stereotypes, or other NSFW contents if used inappropriately. Our model's
        ability to precisely control the generation subject can help mitigate
        certain biases. We can use NSFW detectors to block potential
        inappropriate content from being generated. Nevertheless, we strongly
        caution against using our model directly in user-facing applications
        without a careful inspection of the model's output. Proper content
        moderation and regulation are highly advised to prevent undesirable
        consequence.
      </p>
      <br />
    </div>
    <!-- <div class="content">
      <h2>BibTex</h2>
      <code>
        @article{ruiz2022dreambooth,<br />
        &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion
        Models for Subject-Driven Generation},<br />
        &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun
        and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br />
        &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br />
        &nbsp;&nbsp;year={2022}<br />
        }
      </code>
    </div> -->
    <div class="content" id="acknowledgements">
      <p>
        <strong>Acknowledgement</strong>: We thank colleagues at Salesforce AI
        Research for support and helpful discussion, and authors of
        <a href="https://github.com/CompVis/latent-diffusion">LDM</a>,
        <a href="https://dreambooth.github.io/">DreamBooth</a>,
        <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>,
        <a href="https://prompt-to-prompt.github.io/">Prompt-to-Prompt</a> for
        inspiration. The website design is based on DreamBooth and
        Prompt-to-Prompt project pages.
        <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). -->
      </p>
    </div>
    <script src="./script.js"></script>
  </body>
</html>
